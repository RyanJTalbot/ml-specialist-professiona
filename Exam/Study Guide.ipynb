{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Guide ML-Specialist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Exam Objectives\n",
    "\n",
    "### Section 1 - Evaluate business problem including ethical implications\n",
    "- 1.1 Understand business requirements\n",
    "- 1.2 Understand what data is available\n",
    "- 1.3 Understand ethical challenges in the business problem\n",
    "- 1.4 Perform AI design thinking\n",
    "- 1.5 Assess progress on the AI Ladder\n",
    "\n",
    "\n",
    "### Section 2 - Exploratory Data Analysis including data preparation\n",
    "- 2.1 Identify the methods used to clean, label, and anonymize data\n",
    "- 2.2 Visualize data\n",
    "- 2.3 Balance and partition data\n",
    "\n",
    "\n",
    "### Section 3 - Implement the proper model\n",
    "- 3.1 Implement Supervised Learning: Regression\n",
    "- 3.2 Implement Supervised Learning: Classification\n",
    "- 3.3 Implement Unsupervised Learning: Clustering\n",
    "- 3.4 Implement Unsupervised Learning: Dimensional Reduction\n",
    "\n",
    "\n",
    "### Section 4 - Refine and deploy the model\n",
    "- 4.1 Identify operations and transformations taken to select and engineer features\n",
    "- 4.2 Select the proper tools\n",
    "- 4.3 Configure the appropriate environment specifications for training the model\n",
    "- 4.4 Train the model and optimize hyperparameters\n",
    "- 4.5 Implement the ability for the model to explain itself\n",
    "- 4.6 Deploy the model\n",
    "\n",
    "\n",
    "### Section 5 - Monitor models in production\n",
    "- 5.1 Assess the model\n",
    "- 5.2 Monitor the model in production\n",
    "- 5.3 Determine if there is unfair bias in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Understand business requirements\n",
    "### SUBTASKS:\n",
    "- 1.1.1. Explain how IBM Garage Methodology works\n",
    "- 1.1.2. Understand the CRISP-DM process\n",
    "- 1.1.3. Identify which business opportunity to prioritize and define success metrics for an MVP\n",
    "    - REFERENCES:\n",
    "        - https://www.ibm.com/garage\n",
    "        - https://thinkinsights.net/digital/crisp-dm/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Understand what data is available\n",
    "#### SUBTASKS:\n",
    "- 1.2.1. Use SQL to access data\n",
    "- 1.2.1.1. Extracting specific columns\n",
    "- 1.2.1.2. Filtering data\n",
    "- 1.2.1.3. Combining Tables\n",
    "- 1.2.2. Use Python APIs to access data\n",
    "- 1.2.3. Scrape information from a website\n",
    "- 1.2.4. Read data into a Pandas Dataframe\n",
    "- 1.2.4.1. Reading different types of data assets\n",
    "- 1.2.4.2. Manipulating column names\n",
    "- 1.2.4.3. Obtaining specific rows\n",
    "    - REFERENCES:\n",
    "        - https://www.w3schools.com/sql/\n",
    "        - https://realpython.com/python-api/\n",
    "        - https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "        - https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "        - https://pandas.pydata.org/docs/getting_started/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Understand ethical challenges in the business problem\n",
    "#### SUBTASKS:\n",
    "- 1.3.1. List potential sources of unfair bias\n",
    "- 1.3.2. List potential sources of privacy violations\n",
    "- 1.3.3. List potential secondary and tertiary effects of the application\n",
    "- 1.3.4. Plan to prevent or mitigate negative consequences\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/course/view.php?id=8390\n",
    "        - https://learning.oreilly.com/library/view/ai-fairness/9781492077664/Introduction\n",
    "        - https://www.ibm.com/design/thinking/page/courses/AI_Essentials\n",
    "        - https://www.designethically.com/layers\n",
    "        - https://www.ibm.com/design/ai/ethics/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perform AI design thinking\n",
    "#### SUBTASKS:\n",
    "- 1.4.1. Align on user intents for a solution\n",
    "- 1.4.2. Document the available data\n",
    "- 1.4.3. Determine what training will be required\n",
    "- 1.4.4. Create hypotheses about what the behavior of the system will be\n",
    "- 1.4.5. Assess feasibility and refine if needed\n",
    "- 1.4.6. Consider direct and indirect effects of the solution\n",
    "    - REFERENCES:\n",
    "        - https://www.ibm.com/design/thinking/page/courses/AI_Essentials\n",
    "        - https://www.ibm.com/design/thinking/page/toolkit/activity/ai-essentials-intent\n",
    "        - https://www.ibm.com/design/thinking/static/team-essentials-for-ai-workbook-8dc9aadb2cc2dc6343cc5e420b522ca2.pdf\n",
    "        - https://learning.oreilly.com/library/view/operationalizing-ai/9781098101329/ --- Chapter 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Assess progress on the AI Ladder\n",
    "#### SUBTASKS:\n",
    "- 1.5.1. Assess progress in collecting data\n",
    "- 1.5.2. Assess progress in organizing data\n",
    "- 1.5.3. Assess progress in analyzing data\n",
    "- 1.5.4. Assess progress in infusing AI into the organization\n",
    "    - REFERENCES:\n",
    "        - https://www.ibm.com/downloads/cas/O1VADKY2\n",
    "        - https://learn.ibm.com/course/view.php?id=8496\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.1. Identify the methods used to clean, label, and anonymize data\n",
    "\n",
    "#### SUBTASKS:\n",
    "- 2.1.1. Clean data\n",
    "    - 2.1.1.1. Fill or drop missing values\n",
    "    - 2.1.1.2. Remove duplicate rows\n",
    "    - 2.1.1.3. Remove outliers\n",
    "    - 2.1.1.4. Converting data types\n",
    "    - 2.1.1.5. Data normalization\n",
    "    \n",
    "    \n",
    "- 2.1.2. Label data\n",
    "    - 2.1.2.1. Understand the benefits and challenges to labeling data\n",
    "    - 2.1.2.2. Explain data labeling approaches\n",
    "    \n",
    "    \n",
    "- 2.1.3. Anonymize data\n",
    "    \n",
    "REFERENCES:\n",
    "- https://www.ibm.com/garage/method/practices/reason/prepare-data-for-machine-learning/\n",
    "- https://www.ibm.com/garage/method/practices/code/data-preparation-ai-data-science/\n",
    "- https://www.ibm.com/cloud/learn/data-labeling\n",
    "- https://dataplatform.cloud.ibm.com/docs/content/wsj/governance/dmg22.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1.1. Fill or drop missing values\n",
    "\n",
    "- Remove missing values\n",
    "    - `DataFrame.dropna([axis, how, thresh, ...])`\n",
    "\n",
    "\n",
    "- Fill NA/NaN values using the specified method.\n",
    "    - `DataFrame.fillna([value, method, axis, ...])`\n",
    "\t\n",
    "    \n",
    "- Detect missing values.\n",
    "    - `DataFrame.isna()`\n",
    "\t\n",
    "    \n",
    "- Replace values given in to_replace with value.\t\n",
    "    - `DataFrame.replace([to_replace, value, ...])`\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1.2. Remove duplicate rows\n",
    "\n",
    "- Return DataFrame with duplicate rows removed.\n",
    "    - `new_df = df.drop_duplicates(keep=False, inplace=false)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1.3. Remove outliers\n",
    "\n",
    "#### various ways of outlier detection:\n",
    "- Z-Score\n",
    "    - A z-score simply tells you how many standard deviations away an individual data value falls from the mean.\n",
    "    \n",
    "\n",
    "- IQR-distance from Median\n",
    "    - Interquartile range. The IQR describes the middle 50% of values when ordered from lowest to highest. To find the interquartile range (IQR), first find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1.\n",
    "    \n",
    "    - sklearn's RobustScaler \n",
    "        - Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1.4. Converting data types\n",
    "\n",
    "When working with missing <b>Numerical</b> values:\n",
    "- dropna()\n",
    "    - `df.dropna(subset=['price'])`\n",
    "\n",
    "\n",
    "- drop()\n",
    "    - `df.drop('price', axis=1)`\n",
    "\n",
    "\n",
    "- fillna()\n",
    "    - `median = df['price'].median()\n",
    "      df['price'].fillna(median, inplace=True)`\n",
    "      \n",
    "      \n",
    "- scikit-learn `SimpleImputer`\n",
    "    \n",
    "    `from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')`\n",
    "    \n",
    "    `price_num = df.drop('price', axis=1)\n",
    "    imputer.fit(price_num)`\n",
    "    \n",
    "    `X = imputer.transform(price_num)`\n",
    "    \n",
    "    `transform_df = pd.DataFrame(X, columns=price_num.columns, index=price_num.index`\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working iwth missing <b>Text</b> and <b>Categorica;</b> attributes:\n",
    "\n",
    "\n",
    "- Ordinal Encoding using sci-kit learn `OrdinalEncoder`\n",
    "\n",
    "    `wrd_cat = df[['words']]`\n",
    "\n",
    "    `from sklearn.preprocessing import OrdinalEncoder\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    wrd_cat_encoded = ordinal_encoder.fit_transform(wrd_cat)`\n",
    "    \n",
    "- one-hot encoding using sci-kit learn OneHotEncoder\n",
    "\n",
    "    `wrd_cat = df[['words']]`\n",
    "\n",
    "    `from sklearn.preprocessing import OneHotEncoder\n",
    "    cat_encoder = OneHotEncoder()\n",
    "    wrd_cat_1hot = cat_encoder.fit_transform(wrd_cat)`\n",
    "    \n",
    "    the result is a SciPy sparse matrix. To convert it to a (dense) NumPy array use `toarray()`\n",
    "    \n",
    "    `wrd_cat_1hot.toarray()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1.5. Data normalization\n",
    "\n",
    "- Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1. For machine learning, every dataset does not require normalization. It is required only when features have different ranges.\n",
    "\n",
    "- Min-max scaling (aka normalization) is the simplest form of feature scaling. Values are shifted and rescaled so that they end up ranging from 0 to 1. \n",
    "    - scikit-learn MinMaxScaler\n",
    "\n",
    "\n",
    "- Z Normalization(Standardization):\n",
    "\n",
    "\n",
    "- Unit Vector Normalization:\n",
    "\n",
    "\n",
    "<b>Good practice usage with the MinMaxScaler and other scaling techniques is as follows:</b>\n",
    "\n",
    "- Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function.\n",
    "    \n",
    "    \n",
    "- Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function.\n",
    "    \n",
    "    \n",
    "- Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2. Label data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2.1. Understand the benefits and challenges to labeling data\n",
    "\n",
    "<b>Benefits</b>\n",
    "\n",
    "Data labeling provides users, teams and companies with greater context, quality and usability. More specifically, you can expect:\n",
    "\n",
    "\n",
    "- More Precise Predictions: Accurate data labeling ensures better quality assurance within machine learning algorithms, allowing the model to train and yield the expected output. Otherwise, as the old saying goes, “garbage in, garbage out.” Properly labeled data  provide the “ground truth” (i.e., how labels reflect “real world” scenarios) for testing and iterating subsequent models.\n",
    "\n",
    "\n",
    "- Better Data Usability: Data labeling can also improve usability of data variables within a model. For example, you might reclassify a categorical variable as a binary variable to make it more consumable for a model.  Aggregating data in this way can optimize the model by reducing the number of model variables or enable the inclusion of control variables. Whether you’re using data to build computer vision models (i.e. putting bounding boxes around objects) or NLP models (i.e. classifying text for social sentiment), utilizing high-quality data is a top priority.\n",
    "\n",
    "\n",
    "<b>Challenges</b>\n",
    "\n",
    "Data labeling is not without its challenges. In particular, some of the most common challenges are:\n",
    "\n",
    "- Expensive and time-consuming: While data labeling is critical for machine learning models, it can be costly from both a resource and time perspective. If a business takes a more automated approach, engineering teams will still need to set up data pipelines prior to data processing, and manual labeling will almost always be expensive and time-consuming.\n",
    "\n",
    "\n",
    "- Prone to Human-Error: These labeling approaches are also subject to human-error (e.g. coding errors, manual entry errors), which can decrease the quality of data. This, in turn, leads to inaccurate data processing and modeling. Quality assurance checks are essential to maintaining data quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2.2. Explain data labeling approaches\n",
    "\n",
    "Data labeling is a critical step in developing a high-performance ML model. Though labeling appears simple, it’s not always easy to implement. As a result, companies must consider multiple factors and methods to determine the best approach to labeling. Since each data labeling method has its pros and cons, a detailed assessment of task complexity, as well as the size, scope and duration of the project is advised.\n",
    "\n",
    "<b>Here are some paths to labeling your data:</b>\n",
    "\n",
    "- Internal labeling - Using in-house data science experts simplifies tracking, provides greater accuracy, and increases quality. However, this approach typically requires more time and favors large companies with extensive resources.\n",
    "   \n",
    "   \n",
    "- Synthetic labeling - This approach generates new project data from pre-existing datasets, which enhances data quality and time efficiency. However, synthetic labeling requires extensive computing power, which can increase pricing.\n",
    "\n",
    "\n",
    "- Programmatic labeling - This automated data labeling process uses scripts to reduce time consumption and the need for human annotation. However, the possibility of technical problems requires HITL to remain a part of the quality assurance (QA) process.\n",
    "\n",
    "\n",
    "- Outsourcing - This can be an optimal choice for high-level temporary projects, but developing and managing a freelance-oriented workflow can also be time-consuming. Though freelancing platforms provide comprehensive candidate information to ease the vetting process, hiring managed data labeling teams provides pre-vetted staff and pre-built data labeling tools.\n",
    "\n",
    "\n",
    "- Crowdsourcing - This approach is quicker and more cost-effective due to its micro-tasking capability and web-based distribution. However, worker quality, QA, and project management vary across crowdsourcing platforms. One of the most famous examples of crowdsourced data labeling is Recaptcha. This project was two-fold in that it controlled for bots while simultaneously improving data annotation of images. For example, a Recaptcha prompt would ask a user to identify all the photos containing a car to prove that they were human, and then this program could check itself based on the results of other users. The input of from these users provided a database of labels for an array of images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3. Anonymize data\n",
    "\n",
    "Data anonymization helps you protect sensitive data, such as personally identifiable information or restricted business data to avoid the risk of compromising confidential information. It is defined in policy rules that are enforced for an asset. Depending on the method of data anonymization, data is redacted, masked, or substituted in the asset preview. \n",
    "\n",
    "<b>method to anonymize data:</b>\n",
    "\n",
    "- Redact data values in asset columns.\n",
    "    This method replaces each data value with a string of exactly ten letters of X to remove information that is, for example, identifying or otherwise sensitive. With redacted data, neither the format of the data nor referential integrity is retained.\n",
    "\n",
    "\n",
    "- Substitute data values in asset columns.\n",
    "    This method replaces data with values that don’t match the original format. It preserves referential integrity (RI) to ensure that table relationships are consistent.\n",
    "    If a value is used several times in a column with substituted data, Substitute uses the same substitution value for identical data values.\n",
    "    For example, if a column contains the email address userA@example.com several times, each finding is replaced by the same substitution value, such as: 500ddcc98133703531re3456.\n",
    "\n",
    "\n",
    "- Mask data values in asset columns that contain SSN (US social security numbers) data.\n",
    "    This method replaces data types like SSN with similarly formatted values that match the original format. It does not preserve referential integrity (RI) or data distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Visualize data\n",
    "#### SUBTASKS:\n",
    "\n",
    "\n",
    "- 2.2.1. Choose the column(s) from your dataset to be visualized\n",
    "- 2.2.2. Identify what the visualization should describe about the column(s)\n",
    "    - 2.2.2.1. Distribution\n",
    "    - 2.2.2.2. Correlation\n",
    "    - 2.2.2.3. Comparison\n",
    "    - 2.2.2.4. Time Series\n",
    "- 2.2.3. Select a type of chart based on the descriptive need\n",
    "    - 2.2.3.1. Histogram/Box plot/Violin plot\n",
    "    - 2.2.3.2. Scatterplot/Heatmap\n",
    "    - 2.2.3.3. Bar chart\n",
    "    - 2.2.3.4. Line plot\n",
    "- 2.2.4. Select a library or tool for visualization\n",
    "    - 2.2.4.1. Matplotlib\n",
    "    - 2.2.4.2. Seaborn\n",
    "    - 2.2.4.3. Bokeh\n",
    "    - 2.2.4.4. Plotly\n",
    "- 2.2.5. Plot the visualization\n",
    "\n",
    "\n",
    "REFERENCES:\n",
    "- https://seaborn.pydata.org/introduction.html\n",
    "- https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\n",
    "- https://docs.bokeh.org/en/latest/docs/first_steps.html\n",
    "- https://plotly.com/python/\n",
    "- https://learn.ibm.com/course/view.php?id=8794\n",
    "- https://learning.oreilly.com/library/view/statistics-in-a/9781449361129/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1. Choose the column(s) from your dataset to be visualized\n",
    "\n",
    "Explore data with:\n",
    "- df.info()\n",
    "- df.decribe()\n",
    "- df.shape\n",
    "- df.dtypes\n",
    "- df.head() / df.tail()\n",
    "- df['col']\n",
    "- df.sum()\n",
    "- df.max() / df.min()\n",
    "- df['col'].isnull().sum()\n",
    "- df.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2. Identify what the visualization should describe about the column(s)\n",
    "\n",
    "### 2.2.2.1. Distribution\n",
    "-  Histogram \n",
    "    - DataFrame.plot.hist(by=None, bins=10, **kwargs)\n",
    "    - A histogram is a representation of the distribution of data. This function groups the values of all given Series in the DataFrame into bins \n",
    "\n",
    "- box plot\n",
    "    - Another useful plot the see the data distribution is the box plot. You can simply plot it by using df.plot.box()\n",
    "    \n",
    "- Kernel Density Estimation (KDE) \n",
    "    - plots save you from the hassle of deciding on the bin size by smoothing the histogram. \n",
    "    \n",
    "- Violin plots \n",
    "    - are the perfect combination of the box plots and KDE plots. They deliver the summary statistics with the box plot inside and shape of distribution with the KDE plot on the sides.\n",
    "\n",
    "\n",
    "### 2.2.2.2. Correlation\n",
    "- correlation matrix \n",
    "    - A correlation matrix is a matrix that shows the correlation values of the variables in the dataset. df.corr()\n",
    "    \n",
    "- correlation heatmap \n",
    "    -  plots the correlation as a heatmap. seaborn.heatmap(df.corr())\n",
    "    \n",
    "- Correlation Scatter Plot\n",
    "    - It also supports drawing the linear regression fitting line in the scatter plot. You can enable it or disable it using the fit_reg parameter. By default, the parameter fit_reg is always True which means the linear regression fit line will be plotted by default.\n",
    "    \n",
    "- pair plots\n",
    "\n",
    "### 2.2.2.3. Comparison\n",
    "?????????\n",
    "\n",
    "\n",
    "### 2.2.2.4. Time Series\n",
    "???????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3. Select a type of chart based on the descriptive need\n",
    " \n",
    "### 2.2.3.1. Histogram/Box plot/Violin plot\n",
    "-  Histogram \n",
    "    - DataFrame.plot.hist(by=None, bins=10, **kwargs)\n",
    "    - A histogram is a representation of the distribution of data. This function groups the values of all given Series in the DataFrame into bins \n",
    "\n",
    "- box plot\n",
    "    - Another useful plot the see the data distribution is the box plot. You can simply plot it by using df.plot.box()\n",
    "\n",
    "- Kernel Density Estimation (KDE) \n",
    "    - plots save you from the hassle of deciding on the bin size by smoothing the histogram. \n",
    "\n",
    "- Violin plots \n",
    "    - are the perfect combination of the box plots and KDE plots. They deliver the summary statistics with the box plot inside and shape of distribution with the KDE plot on the sides.\n",
    "\n",
    "### 2.2.3.2. Scatterplot/Heatmap\n",
    "### 2.2.3.3. Bar chart\n",
    "### 2.2.3.4. Line plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4. Select a library or tool for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4.1. Matplotlib\n",
    "- Most of the Matplotlib utilities lies under the pyplot submodule, and are usually imported under the plt alias:\n",
    "    - import matplotlib.pyplot as plt\n",
    "\n",
    "### 2.2.4.2. Seaborn\n",
    "- Seaborn is a library that uses Matplotlib underneath to plot graphs. It will be used to visualize random distributions.\n",
    "- Distplot stands for distribution plot, it takes as input an array and plots a curve corresponding to the distribution of points in the array. \n",
    "    - import matplotlib.pyplot as plt\n",
    "    \n",
    "    import seaborn as sns\n",
    "    \n",
    "    sns.distplot([0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    plt.show() \n",
    "    \n",
    "### 2.2.4.3. Bokeh\n",
    "- Bokeh is a Python library for creating interactive visualizations for modern web browsers. It helps you build beautiful graphics, ranging from simple plots to complex dashboards with streaming datasets. With Bokeh, you can create JavaScript-powered visualizations without writing any JavaScript yourself.\n",
    "\n",
    "### 2.2.4.4. Plotly\n",
    "- Plotly is a javascript library for data visualization. It is based on the famous d3.js library, and provides a python wrapper allowing to build stunning interactive charts directly from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5. Plot the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Balance and partition data\n",
    "\n",
    "#### SUBTASKS:\n",
    "- 2.3.1. Partition data\n",
    "- 2.3.1.1. Create train/test/validation splits\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=165773 (data leakage mentioned in passing)\n",
    "\n",
    "- 2.3.1.2. Understand and implement cross validation\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=166655\n",
    "        - https://learn.ibm.com/mod/page/view.php?id=170328&forceview=1\n",
    "\n",
    "- 2.3.1.3. Prevent data leakage\n",
    "    - REFERENCES:\n",
    "        - https://en.wikipedia.org/wiki/Leakage_(machine_learning)\n",
    "        - https://reproducible.cs.princeton.edu/ (this is a common problem)\n",
    "\n",
    "- 2.3.1.4. Create data splits that are reproducible\n",
    "    - REFERENCES:\n",
    "        - https://cs230.stanford.edu/blog/split/\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=166646&forceview=1\n",
    "        - https://learning.oreilly.com/library/view/machine-learning-design/9781098115777/ch06.html#problem-id00022\n",
    "        \n",
    "- 2.3.2. Balance data\n",
    "\n",
    "- 2.3.2.1. Understand why imbalanced data is problematic\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=167242\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=168614\n",
    "        - https://learn.ibm.com/mod/page/view.php?id=170229&forceview=1\n",
    "\n",
    "- 2.3.2.2. Understand and implement pros, cons, and how to of up-, down-, and re- sampling\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=167243\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=167247\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=167246\n",
    "        - https://learn.ibm.com/mod/page/view.php?id=170230&forceview=1\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=168610&forceview=1\n",
    "\n",
    "- 2.3.2.3. Understand and implement other methods to handle imbalanced data, such as weighting and stratified sampling\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/mod/video/view.php?id=167245\n",
    "        - https://imbalanced-learn.org/stable/index.html (included in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1. Partition data\n",
    "### 2.3.1.1. Create train/test/validation splits\n",
    "- from sklearn.model_selection import train_test_split\n",
    "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    - test_size is the percentage used for the test size\n",
    "    \n",
    "### 2.3.1.2. Understand and implement cross validation\n",
    "- cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using k - 1 of the folds as training data; the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "- The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n",
    "    - from sklearn.model_selection import cross_val_score\n",
    "    - clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "    - scores = cross_val_score(clf, X, y, cv=5)\n",
    "    \n",
    "- other cross validation strategies by passing a cross validation iterator instead, for instance:\n",
    "    - from sklearn.model_selection import ShuffleSplit\n",
    "    - n_samples = X.shape[0]\n",
    "    - cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "    - cross_val_score(clf, X, y, cv=cv)\n",
    "\n",
    "Links: \n",
    "- https://www.kaggle.com/code/alexisbcook/cross-validation\n",
    "- https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "### 2.3.1.3. Prevent data leakage\n",
    "- Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting.\n",
    "\n",
    "- In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets.\n",
    "\n",
    "Ways to Help Prevent Data Leakage:\n",
    "- Understanding the Dataset\n",
    "- Cleaning Dataset for Duplicates\n",
    "- Selecting Features with Regard to Target Variable Correlation and Temporal Ordering\n",
    "- Splitting Dataset into Train, Validation, and Test Groups\n",
    "- Normalizing After Splitting, BUT Before Cross Validation\n",
    "- Assessing Model Performance with a Healthy Skepticism\n",
    "\n",
    "\n",
    "###  2.3.1.4. Create data splits that are reproducible\n",
    "- by using the same test_size and random_state values\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2. Balance data\n",
    "\n",
    "### 2.3.2.1. Understand why imbalanced data is problematic\n",
    "\n",
    "\n",
    "### 2.3.2.2. Understand and implement pros, cons, and how to of up-, down-, and re- sampling\n",
    "   \n",
    "### 2.3.2.3. Understand and implement other methods to handle imbalanced data, such as weighting and stratified sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 – Implement the proper model\n",
    "\n",
    "## 3.1. Implement Supervised Learning: Regression\n",
    "\n",
    "#### SUBTASK(S):\n",
    "\n",
    "- 3.1.1. Describe Regression\n",
    "    - REFERENCES:\n",
    "        - https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba\n",
    "        \n",
    "- 3.1.2. Understand the benefits of Regression\n",
    "    - REFERENCES:\n",
    "        - https://towardsdatascience.com/supervised-learning-the-what-when-why-good-and-bad-part-1-f90e6fe2a606\n",
    "        \n",
    "- 3.1.3. Understand some of the most popular Regression algorithms\n",
    "    - 3.1.3.1. Gradient Boosting Tree\n",
    "    - 3.1.3.2. Neural Network\n",
    "    - 3.1.3.3. Random Forest\n",
    "    - 3.1.3.4. Linear Regression\n",
    "    - 3.1.3.5. Decision Tree\n",
    "        - REFERENCES:\n",
    "            - https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Implement Supervised Learning: Classification\n",
    "#### SUBTASK(S):\n",
    "- 3.2.1. Describe Classification\n",
    "    - REFERENCES:\n",
    "        - https://towardsdatascience.com/supervised-learning-the-what-when-why-good-and-bad-part-1-f90e6fe2a606\n",
    "\n",
    "- 3.2.2. Understand the benefits of Classification\n",
    "    - REFERENCES:\n",
    "        - https://www.javatpoint.com/regression-vs-classification-in-machine-learning\n",
    "        \n",
    "- 3.2.3. Understand some of the most popular Classification algorithms\n",
    "    - 3.2.3.1. Naïve Bayes\n",
    "    - 3.2.3.2. Linear SVM\n",
    "    - 3.2.3.3. Logistic Regression\n",
    "    - 3.2.3.4. K-Nearest Neighbors\n",
    "    - 3.2.3.5. Stochastic Gradient Descent\n",
    "    - 3.2.3.6. Neural Network\n",
    "    - 3.2.3.7. Decision Trees & Random Forest\n",
    "    - 3.2.3.8. Boosting Classifiers\n",
    "        - REFERENCES:\n",
    "            - https://analyticsindiamag.com/7-types-classification-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Implement Unsupervised Learning: Clustering\n",
    "\n",
    "#### SUBTASK(S):\n",
    "\n",
    "- 3.3.1. Describe Clustering\n",
    "    - REFERENCES:\n",
    "        - https://machinelearningmastery.com/clustering-algorithms-with-python/\n",
    "- 3.3.2. Understand the benefits of Clustering\n",
    "    - REFERENCES:\n",
    "        - https://www.explorium.ai/blog/clustering-when-you-should-use-it-and-avoid-it/\n",
    "- 3.3.3. Understand some of the most popular Clustering algorithms\n",
    "- 3.3.3.1. K-means\n",
    "- 3.3.3.2. Gaussian Mixture Model\n",
    "- 3.3.3.3. DBSSCAN\n",
    "    - REFERENCES:\n",
    "        - https://scikit-learn.org/stable/modules/clustering.html\n",
    "    - Additional REFERENCES:\n",
    "        - https://www.statlearning.com/\n",
    "        - http://www.mmds.org/\n",
    "        - https://scikit-learn.org/stable/modules/mixture.html#gmm\n",
    "        - https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf\n",
    "        - https://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Implement Unsupervised Learning: Dimensional Reduction\n",
    "#### SUBTASK(S):\n",
    "- 3.4.1. Describe Dimensional Reduction\n",
    "    - REFERENCES:\n",
    "        - https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/\n",
    "- 3.4.2. Understand the benefits of Dimensional Reduction\n",
    "    - REFERENCES:\n",
    "        - https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/\n",
    "- 3.4.3. Understand some of the most popular Dimensional Reduction Algorithms\n",
    "- 3.4.3.1. Singular Value Decomposition\n",
    "- 3.4.3.2. Latent Dirichlet Analysis\n",
    "- 3.4.3.3. Principal Component Analysis\n",
    "    - REFERENCES:\n",
    "        - https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/\n",
    "    - General Reference for differentiation:\n",
    "        - https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning\n",
    "    - Additional References\n",
    "        - https://www.statlearning.com/\n",
    "        - http://www.mmds.org/\n",
    "        - https://geometria.math.bme.hu/sites/geometria.math.bme.hu/files/users/csgeza/howard-anton-chris-rorres-elementary-linear-algebra-applications-version-11th-edition.pdf\n",
    "        - https://jmlr.org/papers/volume18/14-546/14-546.pdf\n",
    "        - https://en.wikipedia.org/wiki/Curse_of_dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 - Refine and deploy the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Identify operations and transformations taken to select and engineer features\n",
    "#### SUBTASK(S):\n",
    "- 4.1.1. Obtain raw data\n",
    "- 4.1.2. Engineer features using attributes of the raw data\n",
    "- 4.1.3. Use automated techniques to augment and/or select features for use in learning\n",
    "    - REFERENCES:\n",
    "        - https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "        - https://developer.ibm.com/articles/automated-feature-engineering-for-relational-data-with-ibm-autoai/\n",
    "        - https://developer.ibm.com/patterns/model-mgmt-on-watson-studio-local/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Select the proper tools\n",
    "#### SUBTASK(S):\n",
    "- 4.2.1. Identify the tools required based on the:\n",
    "- 4.2.1.1. Model type\n",
    "- 4.2.1.2. Type of data\n",
    "- 4.2.1.3. Feature engineering requirements\n",
    "- 4.2.1.4. Amount of automation desired\n",
    "- 4.2.1.5. Production environment requirements\n",
    "    - REFERENCES:\n",
    "        - https://www.ibm.com/garage/method/practices/reason/evaluate-and-select-machine-learning-algorithm/\n",
    "        - https://developer.ibm.com/articles/cc-models-machine-learning/\n",
    "        - https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_latest/wsj/analyze-data/ml-overview_local.html\n",
    "        - https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_latest/wsj/getting-started/tools.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Configure the appropriate environment specifications for training the model\n",
    "\n",
    "#### SUBTASK(S):\n",
    "- 4.3.1. Identify the frameworks supported by Watson Machine Learning\n",
    "- 4.3.2. Explain the GPU-accelerated computing\n",
    "    - REFERENCES:\n",
    "        - https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-overview.html\n",
    "        - https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/pm_service_supported_frameworks.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Train the model and optimize hyperparameters\n",
    "#### SUBTASK(S):\n",
    "- 4.4.1. Choose and justify the type of algorithm\n",
    "- 4.4.1.1. Regression\n",
    "- 4.4.1.2. Classification\n",
    "- 4.4.1.3. Clustering\n",
    "- 4.4.1.4. Recommendation engines\n",
    "- 4.4.1.5. Anomaly detection\n",
    "- 4.4.2. Describe the trade-offs between underfitting and overfitting a model\n",
    "- 4.4.2.1. Avoid underfitting or overfitting by splitting the data into training, testing, and validation sets\n",
    "- 4.4.3. Compare model parameters and hyperparameters\n",
    "- 4.4.4. Explain hyperparameters and hyperparameter tuning\n",
    "- 4.4.4.1. Tuning is a trial-and-error process\n",
    "- 4.4.4.2. Tuning is based on the training output loss value\n",
    "- 4.4.4.3. Learning rate, number of epochs, hidden layers, hidden units, activation functions\n",
    "- 4.4.5. Summarize search algorithms\n",
    "- 4.4.5.1. Grid Search\n",
    "- 4.4.5.2. Random Search\n",
    "- 4.4.5.3. Bayesian Optimization\n",
    "- 4.4.6. Ensemble multiple models\n",
    "- 4.4.7. Choose and justify the type of algorithm\n",
    "- 4.4.7.1. Regression\n",
    "- 4.4.7.2. Classification\n",
    "- 4.4.7.3. Clustering\n",
    "- 4.4.7.4. Recommendation engines\n",
    "- 4.4.7.5. Anomaly detection\n",
    "    - REFERENCES:\n",
    "        - https://www.ibm.com/garage/method/practices/reason/optimize-train-ai-model/\n",
    "        - https://www.ibm.com/docs/en/wmla/2.2.0?topic=optimization-hyperparameter-search-algorithms\n",
    "        - https://www.ibm.com/garage/method/practices/reason/evaluate-and-select-machine-learning-algorithm/\n",
    "        - https://developer.ibm.com/articles/cc-models-machine-learning/\n",
    "        - https://www.ibm.com/garage/method/practices/reason/evaluate-and-select-machine-learning-algorithm/\n",
    "        - https://developer.ibm.com/articles/cc-models-machine-learning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Implement the ability for the model to explain itself\n",
    "#### SUBTASK(S):\n",
    "- 4.5.1. Determine what user profiles need explanations\n",
    "- 4.5.2. Determine what sort of explanations will make sense to those users\n",
    "- 4.5.3. Select and apply algorithms to generate model explanations\n",
    "- 4.5.3.1. Boolean Decision Rule\n",
    "- 4.5.3.2. Generalized Linear Rule Model\n",
    "- 4.5.3.3. ProfWeight\n",
    "- 4.5.3.4. Teaching Explanations for Decisions (TED)\n",
    "- 4.5.3.5. Contrastive Explanations\n",
    "- 4.5.3.6. Disentangled Inferred Prior VAE\n",
    "- 4.5.3.7. ProtoDash\n",
    "- 4.5.4. Present expalantions in a form that will make sense to the target users\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/course/view.php?id=8717\n",
    "        - https://learn.ibm.com/course/view.php?id=8718\n",
    "        - https://aix360.mybluemix.net/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Deploy the model\n",
    "#### SUBTASK(S):\n",
    "- 4.6.1. Containerize the model with Docker\n",
    "- 4.6.2. Embed the model into Spark\n",
    "- 4.6.3. Deploy the model with Watson Machine Learning\n",
    "    - REFERENCES:\n",
    "        - https://learn.ibm.com/course/view.php?id=8797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Assess the model\n",
    "SUBTASK(S):\n",
    "5.1.1. Distinguish metrics for Classification Models\n",
    "5.1.1.1. Explain how a confusion matrix works\n",
    "5.1.1.2. Explain what AUC measures\n",
    "5.1.1.3. ROC curve\n",
    "Reference for plots with ROC curves:\n",
    "https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf\n",
    "https://synapse.koreamed.org/articles/1027596\n",
    "5.1.1.4. Difference in distance measurements\n",
    "5.1.1.4.1. Manhatta\n",
    "5.1.1.4.2. Euclidean\n",
    "5.1.1.4.3. Cosine similarity\n",
    "REFERENCES:\n",
    "https://learning.oreilly.com/library/view/thoughtful-machine-learning/9781491924129/ Chapter 3, Distances\n",
    "https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
    "5.1.1.5. Understand how tree-based models determine features to split on\n",
    "5.1.2. Distinguish metrics for Regression Models\n",
    "5.1.2.1. How do L1 and L2 Regularization impact the model features\n",
    "5.1.2.2. Understand distinction between Bias and Variance\n",
    "5.1.2.3. What do MSE and R-Squared measure\n",
    "5.1.2.4. Understand common error metrics to evaluate regression models\n",
    "5.1.3. Distinguish metrics for Unsupervised Models\n",
    "5.1.3.1. How do you determine optimal number of K for K-Means Algorithm\n",
    "5.1.3.2. Explain how the inertia metric is calculated\n",
    "5.1.3.3. Explain how the Distortion metric is calculated\n",
    "5.1.3.4. How can you avoid your centroids getting stuck in bad local optima\n",
    "5.1.4. Identify trade-offs between model performance and computational cost\n",
    "5.1.5. Choose the best metric for the model and business problem\n",
    "REFERENCES:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "https://learn.ibm.com/mod/video/view.php?id=166640\n",
    "https://learn.ibm.com/mod/page/view.php?id=170322&forceview=1\n",
    "https://learn.ibm.com/mod/video/view.php?id=166668\n",
    "https://learn.ibm.com/mod/video/view.php?id=166669\n",
    "https://learn.ibm.com/mod/video/view.php?id=166785\n",
    "https://learn.ibm.com/mod/page/view.php?id=170325&forceview=1\n",
    "https://learn.ibm.com/mod/video/view.php?id=166786\n",
    "https://learn.ibm.com/mod/page/view.php?id=170329&forceview=1\n",
    "https://learn.ibm.com/mod/video/view.php?id=169061&forceview=1\n",
    "There is an error in learning material minute 5:10: https://learn.ibm.com/mod/video/view.php?id=166785 The presenter is talking about specificity and the formula for specificity is displayed, but it is incorrectly signed as “Sensitivity”. The next slide has corrected version of formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Monitor the model in production\n",
    "SUBTASK(S):\n",
    "5.2.1. Understand what MLOps is\n",
    "REFERENCES:\n",
    "https://www.ibm.com/blogs/journey-to-ai/2021/04/paving-the-paths-to-ai-engineering-and-modelops/\n",
    "https://ibm-cloud-architecture.github.io/refarch-data-ai-analytics/methodology/MLops/\n",
    "https://learning.oreilly.com/library/view/introducing-mlops/9781492083283/ch01.html\n",
    "5.2.1.1. Understand types of data drift and their impact\n",
    "5.2.2. Monitor model performance metrics using logging\n",
    "https://learn.ibm.com/mod/video/view.php?id=169287\n",
    "https://learn.ibm.com/mod/page/view.php?id=169579&forceview=1\n",
    "https://learn.ibm.com/mod/page/view.php?id=169581&forceview=1\n",
    "https://learn.ibm.com/mod/page/view.php?id=169598&forceview=1\n",
    "5.2.3. Monitor model business KPIs\n",
    "REFERENCES:\n",
    "https://learn.ibm.com/mod/page/view.php?id=170330&forceview=1\n",
    "https://learn.ibm.com/mod/video/view.php?id=169575\n",
    "5.2.4. Decide when to retrain model\n",
    "REFERENCES:\n",
    "https://learning.oreilly.com/library/view/introducing-mlops/9781492083283/ch07.html#online_evaluation – Champion/Challenger section\n",
    "https://learning.oreilly.com/library/view/ml-ops-operationalizing/9781492074663/ch01.html#retraining_and_remodeling - Retraining and remodeling section\n",
    "https://learn.ibm.com/mod/video/view.php?id=169604\n",
    "5.2.5. Use IBM OpenPages to govern models\n",
    "REFERENCES:\n",
    "https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=governance-set-up-model-openpages-mrg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Determine if there is unfair bias in the model\n",
    "SUBTASK(S):\n",
    "5.3.1. Understand how model bias can creep in\n",
    "REFERENCES:\n",
    "https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/\n",
    "https://developer.ibm.com/articles/machine-learning-and-bias/\n",
    "5.3.2. Understand the role of transparency in mitigating bias\n",
    "REFERENCES:\n",
    "https://www.forbes.com/sites/cognitiveworld/2020/05/23/towards-a-more-transparent-ai/?sh=b928073d9371\n",
    "5.3.3. Create an AI FactSheet\n",
    "REFERENCES:\n",
    "https://www.ibm.com/blogs/research/2020/07/aifactsheets/\n",
    "5.3.4. Detect bias in models using IBM AI Fairness 360 Toolkit and Watson OpenScale\n",
    "REFERENCES:\n",
    "https://developer.ibm.com/blogs/ai-fairness-360-raise-ai-right/\n",
    "https://github.com/IBM/bias-mitigation-of-machine-learning-models-using-aif360/blob/main/README.md\n",
    "https://learn.ibm.com/mod/video/view.php?id=168628\n",
    "https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=governance-manage-model-risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "1. Take the IBM Machine Learning Data Scientist v1 assessment test.\n",
    "2. If you pass the assessment exam, visit pearsonvue.com/ibm to schedule your testing sessions.\n",
    "3. If you failed the assessment exam, review how you did by section. Focus attention on the sections where you need improvement. Keep in mind that you can take the assessment exam as many times as you would like <b>($30 per exam)</b>, however, you will still receive <b>the same questions only in a different order.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
